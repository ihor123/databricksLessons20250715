{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "864d30bf-b426-462a-8c42-eecb6b131f94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install websockets nest_asyncio\n",
    "%pip install asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae2416a5-1dc1-45a5-82c7-46685ebe23a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d984a3b-63db-493e-afac-b16151d10ec2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import asyncio\n",
    "import json\n",
    "import websockets\n",
    "from datetime import datetime\n",
    "\n",
    "nest_asyncio.apply()  # Allows nested event loops\n",
    "\n",
    "async def get_btcusdt_prices(n=5):\n",
    "    url = \"wss://stream.binance.com:9443/ws/btcusdt@trade\"\n",
    "    rows = []\n",
    "\n",
    "    async with websockets.connect(url) as websocket:\n",
    "        for _ in range(n):\n",
    "            msg = await websocket.recv()\n",
    "            data = json.loads(msg)\n",
    "            price = float(data['p'])\n",
    "            timestamp = int(data['E'])\n",
    "            system_time = datetime.fromtimestamp(timestamp / 1000.0)\n",
    "            timestamp = int(data['T'])  # Trade time\n",
    "            trade_time = datetime.fromtimestamp(timestamp / 1000.0)\n",
    "            rows.append({\n",
    "                \"price\": price,\n",
    "                \"trade_time\": trade_time,\n",
    "                \"system_time\": system_time\n",
    "            })\n",
    "\n",
    "    return rows\n",
    "\n",
    "# Await the async function directly\n",
    "btc_prices = await get_btcusdt_prices(5)\n",
    "display(btc_prices)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c049a8f-b230-47a8-94b1-1dd520c95056",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import asyncio\n",
    "import json\n",
    "import websockets\n",
    "from datetime import datetime\n",
    "\n",
    "nest_asyncio.apply()  # Allows nested event loops\n",
    "\n",
    "async def get_raw_btcusdt_messages(n=5):\n",
    "    url = \"wss://stream.binance.com:9443/ws/btcusdt@trade\"\n",
    "    raw_messages = []\n",
    "\n",
    "    async with websockets.connect(url) as websocket:\n",
    "        for _ in range(n):\n",
    "            msg = await websocket.recv()  # Keep raw message string\n",
    "            data = json.loads(msg)\n",
    "            data['e1'] = data.pop('e', None)\n",
    "            data['e2'] = data.pop('E', None)\n",
    "            data['t1'] = data.pop('t', None)\n",
    "            data['t2'] = data.pop('T', None)\n",
    "            data['m1'] = data.pop('m', None)\n",
    "            data['m2'] = data.pop('M', None)\n",
    "            raw_messages.append(data)  # Convert to dict for final json array\n",
    "\n",
    "    return json.dumps(raw_messages, indent=2)\n",
    "\n",
    "# Await the async function to get the full JSON string\n",
    "raw_json = await get_raw_btcusdt_messages(5)\n",
    "\n",
    "data_datetime = datetime.now()\n",
    "data_year = data_datetime.year\n",
    "data_month = f\"{data_datetime.month:02d}\"  # pad with 0 if needed\n",
    "data_day = f\"{data_datetime.day:02d}\"      # pad with 0 if needed\n",
    "data_timestamp = data_datetime.timestamp()\n",
    "\n",
    "print(f\"Raw trade data saved to {data_timestamp}\")\n",
    "\n",
    "# Save to DBFS using /dbfs mount\n",
    "output_path = f\"dbfs:/Volumes/workspace/ops/volumes/stockprices/{data_year}/{data_month}/{data_day}/{data_timestamp}/btc_raw_trades.json\"\n",
    "\n",
    "dbutils.fs.put(output_path, raw_json, overwrite=True)\n",
    "\n",
    "print(f\"Raw trade data saved to {output_path}\")\n",
    "\n",
    "output_path = f\"dbfs:/Volumes/workspace/ops/volumes/ingest_stockprices/btc_raw_trades_{data_timestamp}.json\"\n",
    "\n",
    "dbutils.fs.put(output_path, raw_json, overwrite=True)\n",
    "\n",
    "print(f\"Raw trade data saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee080612-bbfc-43b2-b2ac-fa3bd6dce9a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import asyncio\n",
    "import json\n",
    "import websockets\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when,col,current_timestamp\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def get_raw_btcusdt_messages(n=5):\n",
    "    url = \"wss://stream.binance.com:9443/ws/btcusdt@trade\"\n",
    "    raw_messages = []\n",
    "\n",
    "    async with websockets.connect(url) as websocket:\n",
    "        for _ in range(n):\n",
    "            msg = await websocket.recv()\n",
    "            data = json.loads(msg)\n",
    "            # Rename fields\n",
    "            data['e1'] = data.pop('e', None)\n",
    "            data['e2'] = data.pop('E', None)\n",
    "            data['t1'] = data.pop('t', None)\n",
    "            data['t2'] = data.pop('T', None)\n",
    "            data['m1'] = data.pop('m', None)\n",
    "            data['m2'] = data.pop('M', None)\n",
    "            raw_messages.append(data)\n",
    "\n",
    "    return raw_messages  # return list of dicts, not JSON string\n",
    "\n",
    "# Await the async function to get raw messages as list of dicts\n",
    "raw_messages = await get_raw_btcusdt_messages(5)\n",
    "\n",
    "# Prepare output path with integer timestamp\n",
    "data_datetime = datetime.now()\n",
    "data_year = data_datetime.year\n",
    "data_month = f\"{data_datetime.month:02d}\"\n",
    "data_day = f\"{data_datetime.day:02d}\"\n",
    "data_timestamp = int(data_datetime.timestamp())\n",
    "\n",
    "# Convert list of dicts to Spark DataFrame\n",
    "df = spark.createDataFrame(raw_messages).withColumn(\"data_timestamp\",current_timestamp())\n",
    "\n",
    "# Optional: show schema and data\n",
    "df.printSchema()\n",
    "display(df)\n",
    "\n",
    "\n",
    "json_str = df.toPandas().to_json(orient='records')\n",
    "\n",
    "output_path = f\"dbfs:/Volumes/workspace/ops/volumes/stockprices/{data_year}/{data_month}/{data_day}/{data_timestamp}/btc_raw_trades.json\"\n",
    "\n",
    "\n",
    "# Write JSON string to DBFS as a single file\n",
    "dbutils.fs.put(output_path, json_str, overwrite=True)\n",
    "\n",
    "print(f\"Raw trade data saved to {output_path}\")\n",
    "\n",
    "output_path = f\"dbfs:/Volumes/workspace/ops/volumes/ingest_stockprices/btc_raw_trades_{data_timestamp}.json\"\n",
    "\n",
    "dbutils.fs.put(output_path, json_str, overwrite=True)\n",
    "\n",
    "print(f\"Raw trade data saved to {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9390af7b-501d-46a5-ae44-2b155da1fe9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when,col,current_timestamp\n",
    "\n",
    "df=spark.sql(f'''\n",
    "select\n",
    "*,\n",
    "current_timestamp() as processing_time,\n",
    "_metadata.file_name as source_file,\n",
    "_metadata.file_modification_time as source_file_modification_time,\n",
    "_metadata.file_size as source_file_size,\n",
    "_metadata.file_block_start as source_file_block_start,\n",
    "_metadata.file_block_length as source_file_block_length,\n",
    "_metadata.file_path as source_file_path\n",
    "\n",
    "\n",
    "from read_files(\n",
    "  \"dbfs:/Volumes/workspace/ops/volumes\" || \"/orders\",\n",
    "  format => \"json\"\n",
    ");          \n",
    "          \n",
    "          ''')\n",
    "\n",
    "df=(\n",
    "    df.withColumn(\n",
    "        \"TestOne\",\n",
    "        when(col(\"order_id\")>=75129,\"OK\")\n",
    "        .when((col(\"order_id\")<75129) & (col(\"order_id\")>=75125),\"OK1\")\n",
    "        .otherwise(\"NOT OK\")\n",
    "    )\n",
    ")\n",
    "\n",
    "display(df)\n",
    "\n",
    "df.write.format(\"delta\").mode(\"append\").save(\"dbfs:/Volumes/workspace/ops/volumes/resultOrders/2023-07-1/01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc5a5dcd-432c-4491-9db1-ab1e4297db43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.functions import when,col,current_timestamp, input_file_name, input_file_block_length, input_file_block_start \n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType, BooleanType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"e1\", StringType(), True),\n",
    "    StructField(\"e2\", IntegerType(), True),\n",
    "    StructField(\"s\", StringType(), True),\n",
    "    StructField(\"t1\", IntegerType(), True),\n",
    "    StructField(\"p\", DoubleType(), True),\n",
    "    StructField(\"q\", DoubleType(), True),\n",
    "    StructField(\"t2\", IntegerType(), True),\n",
    "    StructField(\"m1\", BooleanType(), True),\n",
    "    StructField(\"m2\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "df=spark.sql(f'''\n",
    "select\n",
    "*,\n",
    "current_timestamp() as processing_time,\n",
    "_metadata.file_name as source_file,\n",
    "_metadata.file_modification_time as source_file_modification_time,\n",
    "_metadata.file_size as source_file_size,\n",
    "_metadata.file_block_start as source_file_block_start,\n",
    "_metadata.file_block_length as source_file_block_length,\n",
    "_metadata.file_path as source_file_path\n",
    "\n",
    "\n",
    "from read_files(\n",
    "  \"dbfs:/Volumes/workspace/ops/volumes/ingest_stockprices/\",\n",
    "  format => \"json\"\n",
    ");    \n",
    "''')\n",
    "\n",
    "df.createOrReplaceTempView(\"etl_bronze_stockprices\")\n",
    "\n",
    "spark.sql(\"select * from etl_bronze_stockprices\").display()\n",
    "\n",
    "spark.sql(\"drop temporary variable if exists test1;\")\n",
    "spark.sql(\"declare variable test1 int;\")\n",
    "spark.sql(\"set variable test1=10;\")          \n",
    "spark.sql(\"select test1\").show()\n",
    "\n",
    "\n",
    "\n",
    "#display(df)\n",
    "\n",
    "#df.write.format(\"delta\").mode(\"append\").save(\"dbfs:/Volumes/workspace/ops/volumes/resultOrders/2023-07-1/01\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96d3a0ea-48cd-4bda-9450-6aeddc95a663",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "--drop temporary variable if exists test1;\n",
    "--declare variable test1 int;\n",
    "\n",
    "--set variable test1 = 10;\n",
    "select test1;\n",
    "\n",
    "--select * from etl_bronze_stockprices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2dbb31b-f1c1-4532-9d16-cf4f95e501f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source_path = \"wasbs://courseware@dbacademy.blob.core.windows.net/data-engineer-learning-path/v03\"\n",
    "destination_path = \"dbfs:/Volumes/workspace/ops/volumes/v03\"\n",
    "\n",
    "# Copy the entire directory recursively\n",
    "dbutils.fs.cp(source_path, destination_path, recurse=True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5174649942120382,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "StockPrices",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
