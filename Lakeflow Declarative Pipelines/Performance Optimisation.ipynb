{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06fb4c09-2cb8-42d8-a9f0-3fb74d70addb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set('spark.databricks.io.cache.enabled', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b75d2be-05fa-4781-a6fc-b979932d5450",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.version\n",
    "##spark.conf.get(\"spark.databricks.clusterUsageTags.clusterName\", \"Not Databricks\")\n",
    "##spark.conf.get(\"spark.databricks.io.cache.enabled\", \"Not Databricks\")\n",
    "spark.conf.get(\"spark.databricks.version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eda3a027-8602-471e-afc9-20e98237d2ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df = (\n",
    "    spark\n",
    "        .range(0,150000000000,1,32)\n",
    "        .select(\n",
    "            hash('id').alias('id'),\n",
    "            rand().alias('value'),\n",
    "            from_unixtime(lit(1701692381+col('id'))).alias('time')\n",
    "        )\n",
    ")\n",
    "\n",
    "##df.display()\n",
    "\n",
    "##df2 = (spark.range(0,2500))\n",
    "##df2.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78053875-57d1-49f2-b78c-9cfcfa102065",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df = (\n",
    "    spark\n",
    "        .range(0,150000000000,1)#,32)\n",
    "        .select(\n",
    "            'id',\n",
    "            round(rand()*1000,2).alias('amount'),\n",
    "            (col('id')%10).alias('country_id'),\n",
    "            (col('id')%100).alias('store_id'),\n",
    "            round(rand()*100000000,0).alias('customer_id'),\n",
    "            from_unixtime(lit(1701692381+col('id'))).alias('time')\n",
    "        )\n",
    ")\n",
    "\n",
    "#df.display()\n",
    "\n",
    "##df2 = (spark.range(0,2500))\n",
    "##df2.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c1a4d83-9745-452a-834b-4eff226fe72a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(df\n",
    " .write\n",
    " .mode('overwrite')\n",
    " .option('overwriteSchema',\"true\")\n",
    " #.option('mergeSchema',\"true\")\n",
    " #.partitionBy('id')\n",
    " .saveAsTable('transactions')\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb36e662-b5f7-49c5-a13a-b11b0b40aabb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "store_df= (spark\n",
    "            .range(0,99)\n",
    "            .select(\n",
    "                'id',\n",
    "                round(rand()*100,0).alias('employees'),\n",
    "                (col('id')%10).alias('country_id'),\n",
    "                expr('uuid()').alias('name')\n",
    "            ))\n",
    "store_df.display()\n",
    "\n",
    "store_df.write.saveAsTable('stores')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "568aeb81-599b-4215-8fc3-70a1aec9b7bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, round, rand, expr, when\n",
    "\n",
    "store_df = (\n",
    "    spark\n",
    "        .range(0, 100000000)\n",
    "        .select(\n",
    "            'id',\n",
    "            round(col('id')%100000,0).alias('customer_master_id'),\n",
    "            round(rand() * 100, 0).alias('customer_band'),\n",
    "            (col('id') % 10).alias('country_id'),\n",
    "            expr('uuid()').alias('name'),\n",
    "            when(round(col('id')%100000,0) == col('id'), True)\n",
    "                .otherwise(False)\n",
    "                .alias('IsCurrent')\n",
    "        )\n",
    ")\n",
    "\n",
    "store_df.display()\n",
    "\n",
    "store_df.write.mode(\"overwrite\").option('overwriteSchema','true').saveAsTable(\"customers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09430cbb-0be0-4f79-a854-d7802b01075e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--select * from customers c\n",
    "--inner join customers cc on c.customer_master_id=cc.id --and cc.IsCurrent=true\n",
    "--where  c.id=2000050\n",
    "\n",
    "select * from customers where customer_master_id=50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9189fe2d-271c-41f1-89e0-8a7d0b3099ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "countries = [\n",
    "    (0,\"Italy\"),\n",
    "    (1,\"France\"),\n",
    "    (2,\"Spain\"),\n",
    "    (3,\"Germany\"),\n",
    "    (4,\"UK\"),\n",
    "    (5,\"USA\"),\n",
    "    (6,\"Canada\"),\n",
    "    (7,\"Mexico\"),\n",
    "    (8,\"Brazil\"),\n",
    "    (9,\"Argentina\"),\n",
    "    (10,\"China\"),\n",
    "    (11,\"Japan\"),\n",
    "    (12,\"Korea\"),\n",
    "    (13,\"India\"),\n",
    "    (14,\"Australia\"),\n",
    "    (15,\"New Zealand\")\n",
    "]\n",
    "\n",
    "columns = [\"id\",\"name\"]\n",
    "\n",
    "countries_df = spark.createDataFrame(data=countries,schema=columns  )\n",
    "\n",
    "countries_df.display()\n",
    "\n",
    "countries_df.write.saveAsTable('countries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f0df943-af82-4766-80f8-9935dbaae6fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select * from iot_data where id=1158307975"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5015ed58-6aa6-4800-9fce-5475d3209368",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select  avg(value) from iot_data where time>=\"2023-12-04 12:19:00\" and time<=\"2023-12-04 13:01:20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d6a7b62-5ad9-43f8-8ae0-48dcdee47329",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"filename\":1500},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755191897249}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "--select _metadata.file_path as filename, min(id),max(id) from iot_data group by _metadata.file_path;\n",
    "\n",
    "select _metadata.file_path as filename, min(time),max(time) from iot_data group by _metadata.file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca8c8d1c-b3ea-4add-a018-1ec0aeaa4ce6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SET use_cached_result = false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53828ce6-fe97-4224-9b0c-4a0fc4c4d4b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\",-1)\n",
    "spark.conf.set(\"spark.databricks.adaptive.autoBroadcastJoinThreshold\",-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbbd8837-2343-4268-8710-d04339e5f0ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_df = spark.sql('''\n",
    "select transactions.id,\n",
    "    amount,\n",
    "    countries.name as country_name,\n",
    "    employees,\n",
    "    stores.name as store_name,\n",
    "    max(cc.name) max_customer_name,\n",
    "    max(cc.customer_band) max_customer_band\n",
    "from transactions as transactions\n",
    "left join /*+ BROADCAST(stores) */\n",
    "    stores on\n",
    "        transactions.store_id=stores.id\n",
    "left join /*+ BROADCAST(stores) */\n",
    "    countries on \n",
    "        transactions.country_id=countries.id\n",
    "left join /*+ BROADCAST(stores) */\n",
    "    customers on\n",
    "        transactions.customer_id=customers.id\n",
    "left join \n",
    "    customers cc on\n",
    "        customers.customer_master_id=cc.id\n",
    "group by transactions.id,\n",
    "    amount,\n",
    "    countries.name,\n",
    "    employees,\n",
    "    stores.name \n",
    "''')\n",
    "\n",
    "joined_df.write.mode('overwrite').option('mergeSchema','true').saveAsTable('transactioned_countries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "605a1722-de2a-435e-94b2-13cadf65f885",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_df = spark.sql('''\n",
    "select transactions.id,\n",
    "    amount,\n",
    "    countries.name as country_name,\n",
    "    employees,\n",
    "    stores.name as store_name,\n",
    "    max(cc.name) max_customer_name,\n",
    "    max(cc.customer_band) max_customer_band\n",
    "from transactions as transactions\n",
    "left join /*+ BROADCAST(stores) */\n",
    "    stores on\n",
    "        transactions.store_id=stores.id\n",
    "left join /*+ BROADCAST(stores) */\n",
    "    countries on \n",
    "        transactions.country_id=countries.id\n",
    "left join /*+ BROADCAST(stores) */\n",
    "    customers on\n",
    "        transactions.customer_id=customers.id\n",
    "left join \n",
    "    customers cc on\n",
    "        customers.customer_master_id=cc.id\n",
    "where stores.employees > 10 and stores.employees<=35 and customers.band between 15 and 65\n",
    "group by transactions.id,\n",
    "    amount,\n",
    "    countries.name,\n",
    "    employees,\n",
    "    stores.name \n",
    "''')\n",
    "\n",
    "joined_df.write.mode('overwrite').option('mergeSchema','true').saveAsTable('transactioned_countries22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69027f23-ef0a-4ded-b6a9-e7b8845a45e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "store_df= (spark\n",
    "            .range(0,99)\n",
    "            .select(\n",
    "                'id',\n",
    "                round(rand()*100,0).alias('employees'),\n",
    "                (col('id')%10).alias('country_id'),\n",
    "                expr('uuid()').alias('name')\n",
    "            ))\n",
    "store_df.display()\n",
    "\n",
    "store_df.write.saveAsTable('stores')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78b83050-35f7-415a-bec2-c9997281749f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select * from countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47d622cb-95b2-4905-ba9a-e34cb799bc2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType, ArrayType, TimestampType, BigIntType,UnixTimestampType,TimestampType\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "schema = StructType([StructField('id',BigIntType(),False),\n",
    "                    StructField('amount',DoubleType(),False),\n",
    "                    StructField('country_id',BigIntType(),False),\n",
    "                    StructField('store_id',BigIntType(),False),\n",
    "                    StructField('customer_id',BigIntType(),False),\n",
    "                    StructField('time',UnixTimestampType,False),\n",
    "                    StructField('timestamp',TimestampType(),False)\n",
    "                    ])\n",
    "empty_df = spark.createDataFrame([],schema=schema)\n",
    "display(empty_df)\n",
    "\n",
    "\n",
    "#empty_df= empty_df.withColumn('timestamp',current_timestamp())\n",
    "\n",
    "empty_df.write.mode(\"merge\").option(\"mergeSchema\",\"true\").saveAsTable(name=\"transactions\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71da6ee8-0ee0-4962-86e6-49f23cbe7b57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef9b97a5-86b1-4556-a9bf-0242139bf44d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--2h\n",
    "--select _metadata.file_path as filename, min(id),max(id) from iot_data group by _metadata.file_path;\n",
    "\n",
    "select _metadata.file_path as filename,_metadata.file_modification_time as file_modification_time,_metadata.file_name as file_name, min(id) as min_id,max(id) as max_id,min(country_id) as min_country_id,max(country_id) as max_country_id,min(store_id) as min_store_id,max(store_id) as max_store_id,\n",
    "min(customer_id) as min_customer_id,max(customer_id) as max_customer_id,count(*) as count_rc\n",
    "from transactions group by _metadata.file_path,_metadata.file_modification_time,_metadata.file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2be0ba8e-f350-48d3-98ff-f2873af5b9b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set('spark.databricks.execution.timeout', timeout_in_seconds)."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8179404845221284,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Performance Optimisation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
