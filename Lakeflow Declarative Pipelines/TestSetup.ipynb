{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d88d10b0-e28b-45de-adb5-26128006fb6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, lit, year, month, dayofmonth, round, rand, col, from_unixtime\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import StringType, TimestampType, BooleanType, StructType, StructField, DoubleType, LongType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "class hp:\n",
    "    catalogue = \"workspace\"\n",
    "    database = \"default\"\n",
    "    display_target = \"development\"\n",
    "\n",
    "    def measuretime(self,sp: SparkSession,statement: str) -> dt.time:\n",
    "        start = datetime.now()\n",
    "        df=sp.sql(statement)\n",
    "        df.display()\n",
    "        end = datetime.now()\n",
    "        return end-start\n",
    "    \n",
    "    def runtest(self,sp: SparkSession,testname: str,statement: str):\n",
    "        print(testname)\n",
    "        dur=self.measuretime(sp,statement)\n",
    "        print(f\"{testname} took {dur.seconds} seconds, or {dur}\")\n",
    "        return\n",
    "\n",
    "    def setup(self, catalogue = \"workspace\", database = \"default\", display_target = \"development\"):\n",
    "        s = None\n",
    "        try:\n",
    "            s = dbutils.widgets.get('catalogue_bronze_name')\n",
    "        except:\n",
    "            print('No catalogue_bronze_name parameter was found')\n",
    "            pass\n",
    "        if s is not None:\n",
    "            self.catalogue = s\n",
    "        s = None\n",
    "        try:\n",
    "            s = dbutils.widgets.get('database_bronze_name')\n",
    "        except:\n",
    "            print('No database_bronze_name parameter was found')\n",
    "            pass\n",
    "        if s is not None:\n",
    "            self.database = s\n",
    "        s = None\n",
    "        try:\n",
    "            s = dbutils.widgets.get('display_target')\n",
    "        except:\n",
    "            print('No display_target parameter was found')\n",
    "            pass\n",
    "        if s is not None:\n",
    "            self.display_target = s\n",
    "        s = None\n",
    "\n",
    "\n",
    "        if catalogue is not None:\n",
    "            self.catalogue = catalogue\n",
    "        if database is not None:\n",
    "            self.database = database\n",
    "        if display_target is not None:\n",
    "            self.display_target = display_target\n",
    "\n",
    "\n",
    "        spark.sql(f'USE CATALOG {self.catalogue}')\n",
    "        spark.sql(f'USE DATABASE {self.database}')\n",
    "\n",
    "\n",
    "        #spark.sql(\"DROP VARIABLE IF EXISTS catalogue_bronze_name\")\n",
    "        #spark.sql(\"DROP VARIABLE IF EXISTS database_bronze_name\")\n",
    "        #spark.sql(\"DROP VARIABLE IF EXISTS display_target\")\n",
    "        #spark.sql(\"DECLARE VARIABLE catalogue_bronze_name STRING\")\n",
    "        #spark.sql(\"DECLARE VARIABLE database_bronze_name STRING\")\n",
    "        #spark.sql(\"DECLARE VARIABLE display_target STRING\")\n",
    "        \n",
    "        spark.sql(\"drop temporary variable if exists catalogue_bronze_name;\")\n",
    "        spark.sql(\"declare variable catalogue_bronze_name string;\")\n",
    "        spark.sql(f\"set variable catalogue_bronze_name='{self.catalogue}';\")          \n",
    "\n",
    "        spark.sql(\"drop temporary variable if exists database_bronze_name;\")\n",
    "        spark.sql(\"declare variable database_bronze_name string;\")\n",
    "        spark.sql(f\"set variable database_bronze_name='{self.database}';\")          \n",
    "\n",
    "        spark.sql(\"drop temporary variable if exists display_target;\")\n",
    "        spark.sql(\"declare variable display_target string;\")\n",
    "        spark.sql(f\"set variable display_target='{self.display_target}';\")          \n",
    "\n",
    "\n",
    "    def __init__(self, catalogue = None, database = None, display_target = None):\n",
    "        #self.setup(catalogue = catalogue, database = database, display_target = display_target)\n",
    "\n",
    "        pass\n",
    "\n",
    "    def add_standard_columns(self, df,createdBy=None,modifiedBy=None):\n",
    "        df = df.withColumn('timestamp', current_timestamp())\n",
    "\n",
    "        if modifiedBy is not None:\n",
    "            df = df.withColumn('modifiedOn', current_timestamp().cast(TimestampType()))\n",
    "            df = df.withColumn('modifiedBy', lit(modifiedBy).cast(StringType()))\n",
    "        else:\n",
    "            df = df.withColumn('modifiedOn', lit(None).cast(TimestampType()))\n",
    "            df = df.withColumn('modifiedBy', lit(None).cast(StringType()))\n",
    "        if createdBy is not None:\n",
    "            df = df.withColumn('createdOn', current_timestamp())\n",
    "            df = df.withColumn('createdBy', lit(createdBy).cast (StringType()))\n",
    "        else:\n",
    "            df = df.withColumn('createdOn', lit(None).cast(TimestampType()))\n",
    "            df = df.withColumn('createdBy', lit(None).cast (StringType()))\n",
    "        \n",
    "        df = df.withColumn('isCurrent', lit(True).cast(BooleanType()))\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def transactions_pt(self, df):\n",
    "        df = (\n",
    "          df\n",
    "            .withColumn('year', year(df['time']))\n",
    "            .withColumn('month', month(df['time']))\n",
    "            .withColumn('customer_partition', df['customer_id']%10)\n",
    "        )\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "chp = hp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2b54a93-8814-4db7-95d9-f3c810001ce2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "USE CATALOG ops;\n",
    "CREATE DATABASE IF NOT EXISTS test2;\n",
    "USE DATABASE test2;\n",
    "--CREATE VOLUME IF NOT EXISTS test1_v;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "271e83ca-8563-4f3a-a6a5-57326ddb1dfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df = (\n",
    "    spark\n",
    "        .range(0,10000000000,1)#,32)\n",
    "        .select(\n",
    "            'id',\n",
    "            round(rand()*1000,2).alias('amount'),\n",
    "            (col('id')%10).alias('country_id'),\n",
    "            (col('id')%100).alias('store_id'),\n",
    "            round(rand()*100000000,0).alias('customer_id'),\n",
    "            from_unixtime(lit(1701692381+col('id'))).alias('time')\n",
    "        )\n",
    ")\n",
    "\n",
    "df = chp.add_standard_columns(df,'etl','mml2')\n",
    "\n",
    "df = chp.transactions_pt(df)\n",
    "\n",
    "(df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"/Volumes/ops/test1/test1_v/transactions\")\n",
    " )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15dda1df-c508-4fae-b57b-c2fd01284342",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df = (\n",
    "    spark\n",
    "        .range(0,10000000000,1)#,32)\n",
    "        .select(\n",
    "            'id',\n",
    "            round(rand()*1000,2).alias('amount'),\n",
    "            (col('id')%10).alias('country_id'),\n",
    "            (col('id')%100).alias('store_id'),\n",
    "            round(rand()*100000000,0).alias('customer_id'),\n",
    "            from_unixtime(lit(1701692381+col('id'))).alias('time')\n",
    "        )\n",
    ")\n",
    "\n",
    "df = chp.add_standard_columns(df,'etl','mml2')\n",
    "\n",
    "df = chp.transactions_pt(df)\n",
    "\n",
    "(df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(['year','month','customer_partition'])\n",
    "    .save(\"/Volumes/ops/test1/test1_v/transactions_2\")\n",
    " )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19c3017c-2b12-4879-b622-f8386a631cd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df = (\n",
    "    spark\n",
    "        .range(0,10000000000,1)#,32)\n",
    "        .select(\n",
    "            'id',\n",
    "            round(rand()*1000,2).alias('amount'),\n",
    "            (col('id')%10).alias('country_id'),\n",
    "            (col('id')%100).alias('store_id'),\n",
    "            round(rand()*100000000,0).alias('customer_id'),\n",
    "            from_unixtime(lit(1701692381+col('id'))).alias('time')\n",
    "        )\n",
    ")\n",
    "\n",
    "df = chp.add_standard_columns(df,'etl','mml2')\n",
    "\n",
    "df = chp.transactions_pt(df)\n",
    "\n",
    "(df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .clusterBy(['year','store_id','customer_partition'])\n",
    "    .save(\"/Volumes/ops/test1/test1_v/transactions_3\")\n",
    " )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f96e5b9-9db1-4f86-aac5-3460f94a82fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df = (\n",
    "    spark\n",
    "        .range(0,10000000000,1)#,32)\n",
    "        .select(\n",
    "            'id',\n",
    "            round(rand()*1000,2).alias('amount'),\n",
    "            (col('id')%10).alias('country_id'),\n",
    "            (col('id')%100).alias('store_id'),\n",
    "            round(rand()*100000000,0).alias('customer_id'),\n",
    "            from_unixtime(lit(1701692381+col('id'))).alias('time')\n",
    "        )\n",
    ")\n",
    "\n",
    "df = chp.add_standard_columns(df,'etl','mml2')\n",
    "\n",
    "df = chp.transactions_pt(df)\n",
    "\n",
    "(df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(['year'])\n",
    "    .clusterBy(['store_id','customer_partition'])\n",
    "    .save(\"/Volumes/ops/test1/test1_v/transactions_4\")\n",
    " )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7d92c17-401a-4eeb-bb58-e0d5f570b87b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select count(*) from transactions;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06f508e3-0106-4b46-bf12-d5f17e7bd415",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "store_df= (spark\n",
    "            .range(0,99)\n",
    "            .select(\n",
    "                'id',\n",
    "                round(rand()*100,0).alias('employees'),\n",
    "                (col('id')%10).alias('country_id'),\n",
    "                expr('uuid()').alias('name')\n",
    "            ))\n",
    "store_df = chp.add_standard_columns(store_df,'etl','mml2')\n",
    "\n",
    "(store_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"/Volumes/ops/test1/test1_v/stores\")\n",
    " )\n",
    "df = spark.read.format(\"delta\").load(\"/Volumes/ops/test1/test1_v/stores\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc0edcfb-7daa-4589-99c0-82eec13d2486",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, round, rand, expr, when\n",
    "\n",
    "store_df = (\n",
    "    spark\n",
    "        .range(0, 100000000)\n",
    "        .select(\n",
    "            'id',\n",
    "            round(col('id')%100000,0).alias('customer_master_id'),\n",
    "            round(rand() * 100, 0).alias('customer_band'),\n",
    "            (col('id') % 10).alias('country_id'),\n",
    "            expr('uuid()').alias('name'),\n",
    "            when(round(col('id')%100000,0) == col('id'), True)\n",
    "                .otherwise(False)\n",
    "                .alias('IsCurrent')\n",
    "        )\n",
    ")\n",
    "store_df = chp.add_standard_columns(store_df,'etl','mml2')\n",
    "(store_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"/Volumes/ops/test1/test1_v/customers\")\n",
    " )\n",
    "df = spark.read.format(\"delta\").load(\"/Volumes/ops/test1/test1_v/customers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ef81a05-74c3-4124-8731-9c8012ed4891",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "countries = [\n",
    "    (0,\"Italy\"),\n",
    "    (1,\"France\"),\n",
    "    (2,\"Spain\"),\n",
    "    (3,\"Germany\"),\n",
    "    (4,\"UK\"),\n",
    "    (5,\"USA\"),\n",
    "    (6,\"Canada\"),\n",
    "    (7,\"Mexico\"),\n",
    "    (8,\"Brazil\"),\n",
    "    (9,\"Argentina\"),\n",
    "    (10,\"China\"),\n",
    "    (11,\"Japan\"),\n",
    "    (12,\"Korea\"),\n",
    "    (13,\"India\"),\n",
    "    (14,\"Australia\"),\n",
    "    (15,\"New Zealand\")\n",
    "]\n",
    "\n",
    "columns = [\"id\",\"name\"]\n",
    "\n",
    "countries_df = spark.createDataFrame(data=countries,schema=columns  )\n",
    "countries_df = chp.add_standard_columns(countries_df,'etl','mml2')\n",
    "\n",
    "(countries_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"/Volumes/ops/test1/test1_v/countries\")\n",
    " )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d62641cd-c806-4dc3-9cbc-e883ce711e34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df = spark.read.format(\"delta\").load(\"/Volumes/ops/test1/test1_v/transactions\")\n",
    "\n",
    "#df.createOrReplaceTempView(\"transactions\")\n",
    "\n",
    "# spark.conf.set(\n",
    "#     \"spark.databricks.delta.retentionDurationCheck.enabled\",\n",
    "#     \"false\"\n",
    "# )\n",
    "\n",
    "spark.sql(\"VACUUM delta.`/Volumes/ops/test1/test1_v/transactions` RETAIN 168 HOURS\")\n",
    "spark.sql(\"OPTIMIZE delta.`/Volumes/ops/test1/test1_v/transactions`\")\n",
    "\n",
    "#df = spark.read.format(\"delta\").load(\"/Volumes/ops/test1/test1_v/countries\")\n",
    "\n",
    "#df.createOrReplaceTempView(\"countries\")\n",
    "\n",
    "# spark.conf.set(\n",
    "#     \"spark.databricks.delta.retentionDurationCheck.enabled\",\n",
    "#     \"false\"\n",
    "# )\n",
    "\n",
    "spark.sql(\"VACUUM delta.`/Volumes/ops/test1/test1_v/countries` RETAIN 168 HOURS\")\n",
    "spark.sql(\"OPTIMIZE delta.`/Volumes/ops/test1/test1_v/countries`\")\n",
    "\n",
    "#df = spark.read.format(\"delta\").load(\"/Volumes/ops/test1/test1_v/customers\")\n",
    "\n",
    "#df.createOrReplaceTempView(\"customers\")\n",
    "\n",
    "# spark.conf.set(\n",
    "#     \"spark.databricks.delta.retentionDurationCheck.enabled\",\n",
    "#     \"false\"\n",
    "# )\n",
    "\n",
    "spark.sql(\"VACUUM delta.`/Volumes/ops/test1/test1_v/customers` RETAIN 168 HOURS\")\n",
    "spark.sql(\"OPTIMIZE delta.`/Volumes/ops/test1/test1_v/customers`\")\n",
    "\n",
    "#df = spark.read.format(\"delta\").load(\"/Volumes/ops/test1/test1_v/stores\")\n",
    "\n",
    "#df.createOrReplaceTempView(\"stores\")\n",
    "\n",
    "# spark.conf.set(\n",
    "#     \"spark.databricks.delta.retentionDurationCheck.enabled\",\n",
    "#     \"false\"\n",
    "# )\n",
    "\n",
    "spark.sql(\"VACUUM delta.`/Volumes/ops/test1/test1_v/stores` RETAIN 168 HOURS\")\n",
    "spark.sql(\"OPTIMIZE delta.`/Volumes/ops/test1/test1_v/stores`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4113af5d-c9d8-4474-82e5-8c9ffc4e2f2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tablea='transactions_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b84b7ebc-a3e8-40c2-99b9-57001a7a9a8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tablea = \"transactions_2\"\n",
    "testname = \"test1.1\"\n",
    "statement=f\"\"\"\n",
    "    select /*+ BROADCAST(stores), BROADCAST(customers), BROADCAST(countries) */\n",
    "count(*) from (\n",
    "    select \n",
    "        transactions.id,\n",
    "        amount,\n",
    "        countries.name as country_name,\n",
    "        employees,\n",
    "        stores.name as store_name,\n",
    "        max(cc.name) as max_customer_name,\n",
    "        max(cc.customer_band) as max_customer_band\n",
    "    from ops.test1.{tablea} as transactions\n",
    "    inner join ops.test1.stores\n",
    "        on transactions.store_id = stores.id\n",
    "    inner join ops.test1.countries\n",
    "        on transactions.country_id = countries.id\n",
    "    inner join ops.test1.customers\n",
    "        on transactions.customer_id = customers.id\n",
    "    inner join ops.test1.customers cc\n",
    "        on customers.customer_master_id = cc.id\n",
    "    group by \n",
    "        transactions.id,\n",
    "        amount,\n",
    "        countries.name,\n",
    "        employees,\n",
    "        stores.name\n",
    "        ) a\n",
    "    \"\"\"\n",
    "chp.runtest(spark,testname,statement)\n",
    "#------------------------------------------------------\n",
    "testname = \"test1.2\"\n",
    "statement=f\"\"\"\n",
    "    select count(*) from (\n",
    "SELECT /*+ BROADCAST(stores), BROADCAST(customers), BROADCAST(countries) */\n",
    "       cc.id,\n",
    "       sum(amount) as amount,\n",
    "       countries.name AS country_name,\n",
    "       employees,\n",
    "       stores.name AS store_name,\n",
    "       MAX(c1.name) AS max_customer_name,\n",
    "       MAX(c1.customer_band) AS max_customer_band\n",
    "FROM ops.test1.{tablea} AS transactions\n",
    "inner JOIN ops.test1.stores\n",
    "    ON transactions.store_id = stores.id\n",
    "inner JOIN ops.test1.countries\n",
    "    ON transactions.country_id = countries.id\n",
    "inner JOIN ops.test1.customers c1\n",
    "    ON transactions.customer_id = c1.id\n",
    "inner JOIN ops.test1.customers cc\n",
    "    ON c1.customer_master_id = cc.id\n",
    "WHERE\n",
    "      transactions.store_id between 10 and 20\n",
    "  AND stores.employees > 10\n",
    "  AND stores.employees <= 35\n",
    "  AND c1.customer_band BETWEEN 15 AND 65\n",
    "GROUP BY cc.id,\n",
    "         countries.name,\n",
    "         employees,\n",
    "         stores.name\n",
    ") a\n",
    "    \"\"\"\n",
    "chp.runtest(spark,testname,statement)\n",
    "#------------------------------------------------------\n",
    "testname = \"test1.3\"\n",
    "statement=f\"\"\"\n",
    "select count(*) from (\n",
    "SELECT /*+ BROADCAST(stores), BROADCAST(customers), BROADCAST(countries) */\n",
    "       transactions.id,\n",
    "       amount,\n",
    "       countries.name AS country_name,\n",
    "       employees,\n",
    "       stores.name AS store_name,\n",
    "       MAX(cc.name) AS max_customer_name,\n",
    "       MAX(cc.customer_band) AS max_customer_band\n",
    "FROM ops.test1.{tablea} AS transactions\n",
    "inner JOIN ops.test1.stores\n",
    "    ON transactions.store_id = stores.id\n",
    "inner JOIN ops.test1.countries\n",
    "    ON transactions.country_id = countries.id\n",
    "inner JOIN ops.test1.customers c1\n",
    "    ON transactions.customer_id = c1.id\n",
    "inner JOIN ops.test1.customers cc\n",
    "    ON c1.customer_master_id = cc.id\n",
    "WHERE\n",
    "      stores.id between 10 and 20\n",
    "  AND stores.employees > 10\n",
    "  AND stores.employees <= 35\n",
    "  AND c1.customer_band BETWEEN 15 AND 65\n",
    "GROUP BY transactions.id,\n",
    "         amount,\n",
    "         countries.name,\n",
    "         employees,\n",
    "         stores.name\n",
    ") a\n",
    "    \"\"\"\n",
    "chp.runtest(spark,testname,statement)\n",
    "#------------------------------------------------------\n",
    "testname = \"test2.1\"\n",
    "statement=f\"\"\"\n",
    "    select \n",
    "count(*) from (\n",
    "    select \n",
    "        transactions.id,\n",
    "        amount,\n",
    "        countries.name as country_name,\n",
    "        employees,\n",
    "        stores.name as store_name,\n",
    "        max(cc.name) as max_customer_name,\n",
    "        max(cc.customer_band) as max_customer_band\n",
    "    from ops.test1.{tablea} as transactions\n",
    "    inner join ops.test1.stores\n",
    "        on transactions.store_id = stores.id\n",
    "    inner join ops.test1.countries\n",
    "        on transactions.country_id = countries.id\n",
    "    inner join ops.test1.customers\n",
    "        on transactions.customer_id = customers.id\n",
    "    inner join ops.test1.customers cc\n",
    "        on customers.customer_master_id = cc.id\n",
    "    group by \n",
    "        transactions.id,\n",
    "        amount,\n",
    "        countries.name,\n",
    "        employees,\n",
    "        stores.name\n",
    "        ) a\n",
    "    \"\"\"\n",
    "chp.runtest(spark,testname,statement)\n",
    "#------------------------------------------------------\n",
    "testname = \"test2.2\"\n",
    "statement=f\"\"\"\n",
    "    select count(*) from (\n",
    "SELECT \n",
    "       cc.id,\n",
    "       sum(amount) as amount,\n",
    "       countries.name AS country_name,\n",
    "       employees,\n",
    "       stores.name AS store_name,\n",
    "       MAX(c1.name) AS max_customer_name,\n",
    "       MAX(c1.customer_band) AS max_customer_band\n",
    "FROM ops.test1.{tablea} AS transactions\n",
    "inner JOIN ops.test1.stores\n",
    "    ON transactions.store_id = stores.id\n",
    "inner JOIN ops.test1.countries\n",
    "    ON transactions.country_id = countries.id\n",
    "inner JOIN ops.test1.customers c1\n",
    "    ON transactions.customer_id = c1.id\n",
    "inner JOIN ops.test1.customers cc\n",
    "    ON c1.customer_master_id = cc.id\n",
    "WHERE\n",
    "      transactions.store_id between 10 and 20\n",
    "  AND stores.employees > 10\n",
    "  AND stores.employees <= 35\n",
    "  AND c1.customer_band BETWEEN 15 AND 65\n",
    "GROUP BY cc.id,\n",
    "         countries.name,\n",
    "         employees,\n",
    "         stores.name\n",
    ") a\n",
    "    \"\"\"\n",
    "chp.runtest(spark,testname,statement)\n",
    "#------------------------------------------------------\n",
    "testname = \"test2.3\"\n",
    "statement=f\"\"\"\n",
    "select count(*) from (\n",
    "SELECT \n",
    "       transactions.id,\n",
    "       amount,\n",
    "       countries.name AS country_name,\n",
    "       employees,\n",
    "       stores.name AS store_name,\n",
    "       MAX(cc.name) AS max_customer_name,\n",
    "       MAX(cc.customer_band) AS max_customer_band\n",
    "FROM ops.test1.{tablea} AS transactions\n",
    "inner JOIN ops.test1.stores\n",
    "    ON transactions.store_id = stores.id\n",
    "inner JOIN ops.test1.countries\n",
    "    ON transactions.country_id = countries.id\n",
    "inner JOIN ops.test1.customers c1\n",
    "    ON transactions.customer_id = c1.id\n",
    "inner JOIN ops.test1.customers cc\n",
    "    ON c1.customer_master_id = cc.id\n",
    "WHERE\n",
    "      stores.id between 10 and 20\n",
    "  AND stores.employees > 10\n",
    "  AND stores.employees <= 35\n",
    "  AND c1.customer_band BETWEEN 15 AND 65\n",
    "GROUP BY transactions.id,\n",
    "         amount,\n",
    "         countries.name,\n",
    "         employees,\n",
    "         stores.name\n",
    ") a\n",
    "    \"\"\"\n",
    "chp.runtest(spark,testname,statement)\n",
    "#------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b06172d1-2d80-4e51-8699-7b7f1692b88b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "drop table ops.test1.countries;\n",
    "drop table ops.test1.customers;\n",
    "drop table ops.test1.stores;\n",
    "drop table ops.test1.transactions_1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd82844f-3b6c-41a6-97d6-03ca5e530f9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create table if not exists ops.test1.countries(\n",
    "  id int not null,\n",
    "  name string not null,\n",
    "  `timestamp` timestamp not null,\n",
    "  `modifiedOn` timestamp,\n",
    "  `modifiedBy` string,\n",
    "  `createdOn` timestamp not null,\n",
    "  `createdBy` string not null,\n",
    "  `isCurrent` boolean not null,\n",
    "  primary key (id)\n",
    ");\n",
    "\n",
    "create table if not exists ops.test1.customers(\n",
    "  id int not null,\n",
    "  customer_master_id int not null,\n",
    "  customer_band int not null,\n",
    "  country_id int not null,\n",
    "  name string not null,\n",
    "  `modifiedOn` timestamp,\n",
    "  `modifiedBy` string,\n",
    "  `createdOn` timestamp not null,\n",
    "  `createdBy` string not null,\n",
    "  `isCurrent` boolean not null,\n",
    "  primary key (id)\n",
    ");\n",
    "\n",
    "create table if not exists ops.test1.stores(  \n",
    "  id int not null,\n",
    "  employees int not null,\n",
    "  country_id int not null,\n",
    "  name string not null,\n",
    "  `modifiedOn` timestamp,\n",
    "  `modifiedBy` string,\n",
    "  `createdOn` timestamp not null,\n",
    "  `createdBy` string not null,\n",
    "  `isCurrent` boolean not null,\n",
    "  primary key (id)\n",
    ");\n",
    "\n",
    "create table if not exists ops.test1.transactions_1(\n",
    "  id bigint not null,\n",
    "  amount double not null,\n",
    "  country_id int not null,\n",
    "  store_id int not null,\n",
    "  customer_id int not null,\n",
    "  `time` timestamp not null,\n",
    "  `timestamp` timestamp not null,\n",
    "  `modifiedOn` timestamp,\n",
    "  `modifiedBy` string,\n",
    "  `createdOn` timestamp not null,\n",
    "  `createdBy` string not null,\n",
    "  `isCurrent` boolean not null,\n",
    "  `year` smallint not null,\n",
    "  `month` tinyint not null,\n",
    "  `customer_partition` smallint not null,\n",
    "  primary key (id)\n",
    ");\n",
    "\n",
    "\n",
    "ALTER TABLE ops.test1.transactions_1\n",
    "ADD CONSTRAINT fk_ops_test1_transactions_1_countries FOREIGN KEY (country_id) REFERENCES ops.test1.countries(id);\n",
    "ALTER TABLE ops.test1.transactions_1\n",
    "ADD CONSTRAINT fk_ops_test1_transactions_1_customers FOREIGN KEY (customer_id) REFERENCES ops.test1.customers(id);\n",
    "ALTER TABLE ops.test1.transactions_1\n",
    "ADD CONSTRAINT fk_ops_test1_transactions_1_stores FOREIGN KEY (store_id) REFERENCES ops.test1.stores(id);\n",
    "\n",
    "/*\n",
    "insert into ops.test1.stores(id, employees, country_id, name, `modifiedOn`, `modifiedBy`, `createdOn`, `createdBy`, `isCurrent`)\n",
    "select id, employees, country_id, name, `modifiedOn`, `modifiedBy`, `createdOn`, `createdBy`, `isCurrent`\n",
    "from stores;\n",
    "insert into ops.test1.countries(id, name, `timestamp`, `modifiedOn`, `modifiedBy`, `createdOn`, `createdBy`, `isCurrent`)\n",
    "select id, name, `timestamp`, `modifiedOn`, `modifiedBy`, `createdOn`, `createdBy`, `isCurrent`\n",
    "from countries;\n",
    "insert into ops.test1.customers(id, customer_master_id, customer_brand, country_id, name, `modifiedOn`, `modifiedBy`, `createdOn`, `createdBy`, `isCurrent`)\n",
    "select id, customer_master_id, customer_brand, country_id, name, `modifiedOn`, `modifiedBy`, `createdOn`, `createdBy`, `isCurrent`\n",
    "from customers;\n",
    "insert into ops.test1.transactions_1(id, country_id, store_id, customer_id, `time`, `timestamp`, `modifiedOn`, `modifiedBy`, `createdOn`, `createdBy`, `isCurrent`, `year`, `month`, `customer_partition`)\n",
    "select id, country_id, store_id, customer_id, `time`, `timestamp`, `modifiedOn`, `modifiedBy`, `createdOn`, `createdBy`, `isCurrent`, `year`, `month`, `customer_partition`\n",
    "from transactions;\n",
    "*/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bca3068c-1b6b-451e-8c6c-df655142b57d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "drop table if exists ops.test1.transactions_2;\n",
    "drop table if exists ops.test1.transactions_3;\n",
    "drop table if exists ops.test1.transactions_4;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5c1c170-d431-44a9-85c1-7732442b06f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create table if not exists ops.test1.transactions_2(\n",
    "  id bigint not null,\n",
    "  amount double not null,\n",
    "  country_id int not null,\n",
    "  store_id int not null,\n",
    "  customer_id int not null,\n",
    "  `time` timestamp not null,\n",
    "  `timestamp` timestamp not null,\n",
    "  `modifiedOn` timestamp,\n",
    "  `modifiedBy` string,\n",
    "  `createdOn` timestamp not null,\n",
    "  `createdBy` string not null,\n",
    "  `isCurrent` boolean not null,\n",
    "  `year` smallint not null,\n",
    "  `month` tinyint not null,\n",
    "  `customer_partition` smallint not null,\n",
    "   constraint pk_ops_test1_transactions_2 primary key(id)\n",
    ") using delta tblproperties(\n",
    "  'delta.feature.allowColumnDefaults' = 'supported',\n",
    "  'delta.columnMapping.mode' = 'name'\n",
    " ) partitioned by (year,month,customer_partition);\n",
    "\n",
    "\n",
    "ALTER TABLE ops.test1.transactions_2\n",
    "ADD CONSTRAINT fk_ops_test1_transactions_2_countries FOREIGN KEY (country_id) REFERENCES ops.test1.countries(id);\n",
    "ALTER TABLE ops.test1.transactions_2\n",
    "ADD CONSTRAINT fk_ops_test1_transactions_2_customers FOREIGN KEY (customer_id) REFERENCES ops.test1.customers(id);\n",
    "ALTER TABLE ops.test1.transactions_2\n",
    "ADD CONSTRAINT fk_ops_test1_transactions_2_stores FOREIGN KEY (store_id) REFERENCES ops.test1.stores(id);\n",
    "\n",
    "\n",
    "\n",
    "create table if not exists ops.test1.transactions_3(\n",
    "  id bigint not null,\n",
    "  amount double not null,\n",
    "  country_id int not null,\n",
    "  store_id int not null,\n",
    "  customer_id int not null,\n",
    "  `time` timestamp not null,\n",
    "  `timestamp` timestamp not null,\n",
    "  `modifiedOn` timestamp,\n",
    "  `modifiedBy` string,\n",
    "  `createdOn` timestamp not null,\n",
    "  `createdBy` string not null,\n",
    "  `isCurrent` boolean not null,\n",
    "  `year` smallint not null,\n",
    "  `month` tinyint not null,\n",
    "  `customer_partition` smallint not null,\n",
    "  constraint pk_ops_test1_transactions_3 primary key(id)\n",
    ") using delta tblproperties(\n",
    "  'delta.feature.allowColumnDefaults' = 'supported',\n",
    "  'delta.columnMapping.mode' = 'name'\n",
    " ) cluster by (year,store_id,customer_partition);\n",
    "\n",
    "\n",
    "ALTER TABLE ops.test1.transactions_3\n",
    "ADD CONSTRAINT fk_ops_test1_transactions_3_countries FOREIGN KEY (country_id) REFERENCES ops.test1.countries(id);\n",
    "ALTER TABLE ops.test1.transactions_3\n",
    "ADD CONSTRAINT fk_ops_test1_transactions_3_customers FOREIGN KEY (customer_id) REFERENCES ops.test1.customers(id);\n",
    "ALTER TABLE ops.test1.transactions_3\n",
    "ADD CONSTRAINT fk_ops_test1_transactions_3_stores FOREIGN KEY (store_id) REFERENCES ops.test1.stores(id);\n",
    "\n",
    "create table if not exists ops.test1.transactions_4(\n",
    "  id bigint not null,\n",
    "  amount double not null,\n",
    "  country_id int not null,\n",
    "  store_id int not null,\n",
    "  customer_id int not null,\n",
    "  `time` timestamp not null,\n",
    "  `timestamp` timestamp not null,\n",
    "  `modifiedOn` timestamp,\n",
    "  `modifiedBy` string,\n",
    "  `createdOn` timestamp not null,\n",
    "  `createdBy` string not null,\n",
    "  `isCurrent` boolean not null,\n",
    "  `year` smallint not null,\n",
    "  `month` tinyint not null,\n",
    "  `customer_partition` smallint not null,\n",
    "  constraint pk_ops_test1_transactions_4 primary key(id)\n",
    ") using delta tblproperties(\n",
    "  'delta.feature.allowColumnDefaults' = 'supported',\n",
    "  'delta.columnMapping.mode' = 'name'\n",
    " ) partitioned by (year)\n",
    " cluster by (store_id,customer_partition);\n",
    "\n",
    "\n",
    "\n",
    "ALTER TABLE ops.test1.transactions_4\n",
    "ADD CONSTRAINT fk_ops_test1_transactions_4_countries FOREIGN KEY (country_id) REFERENCES ops.test1.countries(id);\n",
    "ALTER TABLE ops.test1.transactions_4\n",
    "ADD CONSTRAINT fk_ops_test1_transactions_4_customers FOREIGN KEY (customer_id) REFERENCES ops.test1.customers(id);\n",
    "ALTER TABLE ops.test1.transactions_4\n",
    "ADD CONSTRAINT fk_ops_test1_transactions_4_stores FOREIGN KEY (store_id) REFERENCES ops.test1.stores(id);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14a0b6a8-4e98-4829-b113-173a440947cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df = (\n",
    "    spark\n",
    "        .range(0,10000000000,1)#,32)\n",
    "        .select(\n",
    "            'id',\n",
    "            round(rand()*1000,2).alias('amount'),\n",
    "            (col('id')%10).alias('country_id'),\n",
    "            (col('id')%100).alias('store_id'),\n",
    "            round(rand()*100000000,0).alias('customer_id'),\n",
    "            from_unixtime(lit(1701692381+col('id'))).alias('time')\n",
    "        )\n",
    ")\n",
    "\n",
    "df = chp.add_standard_columns(df,'etl','mml2')\n",
    "\n",
    "df = chp.transactions_pt(df)\n",
    "\n",
    "(df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(['year','month','customer_partition'])\n",
    "    .save(\"/Volumes/ops/test1/test1_v/transactions_2\")\n",
    " )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "371d149d-5d09-4ca7-bff4-fe6f15ee3394",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "create table if not exists ops.test1.transactions(\n",
    "  id bigint not null,\n",
    "  amount double not null,\n",
    "  country_id int not null,\n",
    "  store_id int not null,\n",
    "  customer_id int not null,\n",
    "  `time` timestamp not null,\n",
    "  `timestamp` timestamp not null,\n",
    "  `modifiedOn` timestamp,\n",
    "  `modifiedBy` string,\n",
    "  `createdOn` timestamp not null,\n",
    "  `createdBy` string not null,\n",
    "  `isCurrent` boolean not null,\n",
    "  `year` smallint not null,\n",
    "  `month` tinyint not null,\n",
    "  `customer_partition` smallint not null,\n",
    "  primary key (id)\n",
    ")\n",
    "LOCATION '/Volumes/ops/test1/test1_v/transactions';\n",
    "\n",
    "\n",
    "ALTER TABLE ops.test1.transactions\n",
    "ADD CONSTRAINT fk_ops_test1_transactions_1_countries FOREIGN KEY (country_id) REFERENCES ops.test1.countries(id);\n",
    "ALTER TABLE ops.test1.transactions\n",
    "ADD CONSTRAINT fk_ops_test1_transactions_1_customers FOREIGN KEY (customer_id) REFERENCES ops.test1.customers(id);\n",
    "ALTER TABLE ops.test1.transactions\n",
    "ADD CONSTRAINT fk_ops_test1_transactions_1_stores FOREIGN KEY (store_id) REFERENCES ops.test1.stores(id);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d1f6276-7dc2-4994-ae15-eb1c08b954d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "target_table = DeltaTable.forName(spark, \"ops.test1.transactions_2\")\n",
    "source_df = spark.read.format(\"delta\").load(\"/Volumes/ops/test1/test1_v/transactions_2\")\n",
    "\n",
    "(\n",
    "    target_table.alias(\"t\")\n",
    "    .merge(\n",
    "        source=source_df.alias(\"s\"),\n",
    "        condition=\"t.id = s.id\"\n",
    "    )\n",
    "    .whenMatchedUpdateAll()\n",
    "    .whenNotMatchedInsertAll()\n",
    "    .execute()\n",
    ")\n",
    "\n",
    "\n",
    "target_table = DeltaTable.forName(spark, \"ops.test1.transactions_3\")\n",
    "source_df = spark.read.format(\"delta\").load(\"/Volumes/ops/test1/test1_v/transactions_3\")\n",
    "\n",
    "(\n",
    "    target_table.alias(\"t\")\n",
    "    .merge(\n",
    "        source=source_df.alias(\"s\"),\n",
    "        condition=\"t.id = s.id\"\n",
    "    )\n",
    "    .whenMatchedUpdateAll()\n",
    "    .whenNotMatchedInsertAll()\n",
    "    .execute()\n",
    ")\n",
    "\n",
    "\n",
    "target_table = DeltaTable.forName(spark, \"ops.test1.transactions_4\")\n",
    "source_df = spark.read.format(\"delta\").load(\"/Volumes/ops/test1/test1_v/transactions__4\")\n",
    "\n",
    "(\n",
    "    target_table.alias(\"t\")\n",
    "    .merge(\n",
    "        source=source_df.alias(\"s\"),\n",
    "        condition=\"t.id = s.id\"\n",
    "    )\n",
    "    .whenMatchedUpdateAll()\n",
    "    .whenNotMatchedInsertAll()\n",
    "    .execute()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd50f46b-99bb-4caa-a4fe-a3151acdfcc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "target_table = DeltaTable.forName(spark, \"ops.test1.stores\")\n",
    "source_df = spark.read.format(\"delta\").load(\"/Volumes/ops/test1/test1_v/stores\")\n",
    "\n",
    "(\n",
    "    target_table.alias(\"t\")\n",
    "    .merge(\n",
    "        source=source_df.alias(\"s\"),\n",
    "        condition=\"t.id = s.id\"\n",
    "    )\n",
    "    .whenMatchedUpdateAll()\n",
    "    .whenNotMatchedInsertAll()\n",
    "    .execute()\n",
    ")\n",
    "\n",
    "target_table = DeltaTable.forName(spark, \"ops.test1.customers\")\n",
    "source_df = spark.read.format(\"delta\").load(\"/Volumes/ops/test1/test1_v/customers\")\n",
    "\n",
    "(\n",
    "    target_table.alias(\"t\")\n",
    "    .merge(\n",
    "        source=source_df.alias(\"s\"),\n",
    "        condition=\"t.id = s.id\"\n",
    "    )\n",
    "    .whenMatchedUpdateAll()\n",
    "    .whenNotMatchedInsertAll()\n",
    "    .execute()\n",
    ")\n",
    "\n",
    "target_table = DeltaTable.forName(spark, \"ops.test1.countries\")\n",
    "source_df = spark.read.format(\"delta\").load(\"/Volumes/ops/test1/test1_v/countries\")\n",
    "\n",
    "(\n",
    "    target_table.alias(\"t\")\n",
    "    .merge(\n",
    "        source=source_df.alias(\"s\"),\n",
    "        condition=\"t.id = s.id\"\n",
    "    )\n",
    "    .whenMatchedUpdateAll()\n",
    "    .whenNotMatchedInsertAll()\n",
    "    .execute()\n",
    ")\n",
    "\n",
    "target_table = DeltaTable.forName(spark, \"ops.test1.transactions_1\")\n",
    "source_df = spark.read.format(\"delta\").load(\"/Volumes/ops/test1/test1_v/transactions\")\n",
    "\n",
    "(\n",
    "    target_table.alias(\"t\")\n",
    "    .merge(\n",
    "        source=source_df.alias(\"s\"),\n",
    "        condition=\"t.id = s.id\"\n",
    "    )\n",
    "    .whenMatchedUpdateAll()\n",
    "    .whenNotMatchedInsertAll()\n",
    "    .execute()\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57aeee7c-4e46-4449-a346-62f9c22ca359",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from ops.test1.stores"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5302813655859649,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "TestSetup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
